---
title: "DM_Excersize2_Q1"
author: "Joey Herrera"
date: "2/10/2021"
output: pdf_document
---

```{r setup, include=FALSE, message=FALSE}
library(tidyverse)
library(ggplot2)
library(lubridate)
library(rsample)  # for creating train/test splits
library(caret)
library(modelr) 
library(parallel) #run things at the sametime
library(foreach) #loop
library(mosaic) #SaratogaHouses dataset located here
library(gmodels)

capmetro = read_csv('/Users/josephherrera/desktop/ECO395M/data/capmetro_UT.csv')
```

# Data Mining Assignment 2

### Question 1: Data Visualization

1A. One panel of line graphs that plots average boardings grouped by hour of the day, day of week, and month. You should facet by day of week. Each facet should include three lines, one for each month, colored differently and with colors labeled with a legend. 

Give the figure an informative caption in which you explain what is shown in the figure and address the following questions, citing evidence from the figure. Does the hour of peak boardings change from day to day, or is it broadly similar across days? Why do you think average boardings on Mondays in September look lower, compared to other days and months? Similarly, why do you think average boardings on Weds/Thurs/Fri in November look lower?
```{r echo=FALSE, message=FALSE, warning=FALSE}
#Generate a variable calculating the average boardings group
capmetro_timestamp = capmetro %>%
  mutate(day_of_week = factor(day_of_week,
                 levels=c("Mon", "Tue", "Wed","Thu", "Fri", "Sat", "Sun")),
               month = factor(month,
                 levels=c("Sep", "Oct","Nov")),
         timestamp = ymd_hms(timestamp)) %>%
    group_by(hour_of_day, day_of_week, month) %>%
  summarise( avg_boarding = mean(boarding))

#Dummy variable encoding
capmetro_matrix = capmetro_timestamp %>%
  model_matrix(~avg_boarding + month)

#Join both matricies by avg_boarding
capmetro_facet <- merge(capmetro_timestamp, capmetro_matrix, by="avg_boarding")
#Add an additional monthSep var so I dont need to graph the intercept
capmetro_facet = capmetro_facet %>%
  mutate(monthSep = ifelse(monthOct & monthNov == 0, 0 ,1)) #The zeros correpsond to the values that monthOct and monthNov must be for monthSep to equal 1

# Create the first faceted line plot
ggplot(data = capmetro_facet) +
  geom_line(aes(x= hour_of_day, y= avg_boarding, color = month)) + #Color can add an addtional variable to the plot
  facet_wrap(~ day_of_week) +
  xlab("Hour of the Day") +
  ylab("Average Boarding") +
  theme_bw()
```
Caption: The above facet plots depict the average number of passengers on Capmetro buses at UT by the hour of the day, day of the week, and relevant month of the year.

Does the hour of peak boardings change from day to day, or is it broadly similar across days?
During peak hours of the day bus boardings are very similar during the week. Average boarding significantly decreases during the weekend since UT students do not have class. 

Why do you think average boardings on Mondays in September look lower, compared to other days and months?
Compared to other weekdays and months, the average student bus boarding are smaller. This is because of Labor Day, which UT students have off from class. The main reason why students board the bus to campus is to attend class, if you average in Labor Day with the other Monday's in September, the average number of boardings will decrease significantly.

Why do you think average boardings on Weds/Thurs/Fri in November look lower? The average boardings on Wed/Thurs/Fri are also lower because of a holiday break. Typically, UT students recieve Weds/Thurs/Fri off during the week of Thanksgiving.




\newpage
1B. One panel of scatter plots showing boardings (y) vs. temperature (x) in each 15-minute window, faceted by hour of the day, and with points colored in according to whether it is a weekday or weekend. 

Give the figure an informative caption in which you explain what is shown in the figure and answer the following question, citing evidence from the figure. When we hold hour of day and weekend status constant, does temperature seem to have a noticeable effect on the number of UT students riding the bus?

```{r echo=FALSE, warning=FALSE, message=FALSE}
capmetro_scatter = capmetro %>%
  mutate(day_of_week = factor(day_of_week,
                 levels=c("Mon", "Tue", "Wed","Thu", "Fri", "Sat", "Sun")),
               month = factor(month,
                 levels=c("Sep", "Oct","Nov")),
         timestamp = ymd_hms(timestamp)) %>%
  group_by(timestamp, weekend) 

ggplot(data = capmetro_scatter) +
  geom_point(aes(x=temperature, y=boarding, color = weekend)) +
  facet_wrap(~ hour_of_day)
```
Caption: The facet plots aboce show the temperature's effect on average bus boarding for capmetro at UT for each hour of the day and weekday versus weekend.

When we hold hour of day and weekend status constant, does temperature seem to have a noticeable effect on the number of UT students riding the bus?

The facted plots above illustrate that temperature does not have a noticable effect on the number of UT students riding the bus when holding hour of the day and weekend status constant. There seem to be a couple of outliers on the 16th hour of a weekday (4 pm) around 60 degrees farenehit, but these observations seem insignificant.

\newpage
## Question 2: Saratoga Housing Prices

2A. Return to the data set on house prices in Saratoga, NY that we considered in class. Recall that a starter script here is in saratoga_lm.R. For this data set, you'll run a "horse race" (i.e. a model comparison exercise) between two model classes: linear models and KNN.

Build the best linear model for price that you can. It should clearly outperform the "medium" model that we considered in class. Use any combination of transformations, engineering features, polynomial terms, and interactions that you want; and use any strategy for selecting the model that you want.

Answer: The goal of this question is to compare an optimal KNN with an optimal linear model and the "medium" model we considered in class. I will compare the RMSE of each model to each other where the lower the RMSE the closer to the actual housing price the prediction is. To begin, I standardized my variables to account for the large variation in price. Next, I estimated twenty samples of the medium model and took the average of its RMSE as a measure to compare the KNN and optimal linear model with. The RMSE for the medium model can be found below.
```{r echo=FALSE, warning=FALSE, message=FALSE}
#Load Saratoga houses data
data(SaratogaHouses)

#Standardize the variables
SaratogaHouses = SaratogaHouses %>%
   mutate_at(c('price', 'lotSize', 'age', 'landValue', 'livingArea', 'pctCollege', 'bedrooms', 'bathrooms', 'fireplaces', 'rooms'), funs(c(scale(.))))

#Linear models from class
# Split into training and testing sets
saratoga_split = initial_split(SaratogaHouses, prop = 0.8)
saratoga_train = training(saratoga_split)
saratoga_test = testing(saratoga_split)
	
# Fit the medium model to the training data
saratoga_split_lm = initial_split(SaratogaHouses, prop = 0.8)
saratoga_train_lm = training(saratoga_split_lm)
saratoga_test_lm = testing(saratoga_split_lm)

rmse_out_lm = foreach(i=1:20, .combine='rbind') %dopar% {
saratoga_split_lm = initial_split(SaratogaHouses, prop = 0.8)
saratoga_train_lm = training(saratoga_split_lm)
saratoga_test_lm = testing(saratoga_split_lm)
    # train the model and calculate RMSE on the test set
 this_rmse_lm = lm(price ~ . - pctCollege - sewer - waterfront - landValue - newConstruction, data=saratoga_train_lm)
    modelr::rmse(this_rmse_lm, saratoga_test_lm)
}
rmse_out_lm_mean = mean(rmse_out_lm)
rmse_out_lm_mean

```
Next, I hand created a linear model using trial and error to put the main effects that gave the model its lowest RMSE. Then, I added a few interactions with the living area variable to other variables that seem dependent on it such as, rooms, bathrooms, and fireplaces. Finally, I took twenty samples of my best linear model to get a standardize RMSE below, which narrowly beats the medium linear model above.
```{r echo=FALSE, warning=FALSE, message=FALSE}



#Add additional variables to the best linear model (BLM)
SaratogaHouses = SaratogaHouses %>%
  mutate(livingArea_rooms = livingArea*rooms,
         livingArea_bathrooms = livingArea*bathrooms,
         livingArea_fireplaces = livingArea*fireplaces)
        # log_age = log(age)) #cannot use log_age because some observation have age = 0


#Create the best linear model for price
#Add additional variables to the best linear model (BLM)
saratoga_split_blm = initial_split(SaratogaHouses, prop = 0.8)
saratoga_train_blm = training(saratoga_split_blm)
saratoga_test_blm = testing(saratoga_split_blm)

rmse_out_blm = foreach(i=1:20, .combine='rbind') %dopar% {
saratoga_split_blm = initial_split(SaratogaHouses, prop = 0.8)
saratoga_train_blm = training(saratoga_split_blm)
saratoga_test_blm = testing(saratoga_split_blm)
    # train the model and calculate RMSE on the test set
    blm_model = lm(price ~ lotSize + landValue + age + livingArea + bedrooms + fireplaces + bathrooms + rooms + heating + fuel + centralAir + livingArea_rooms+ livingArea_bathrooms + livingArea_fireplaces, data=saratoga_train_blm)
      this_rmse_blm = modelr::rmse(blm_model, saratoga_test_blm)
  }

rmse_out_blm_mean = mean(rmse_out_blm)
rmse_out_blm_mean
```

\newpage
2B. Now build the best K-nearest-neighbor regression model for price that you can. Note: you still need to choose which features should go into a KNN model, but you don't explicitly include interactions or polynomial terms. The method is sufficiently adaptable to find interactions and nonlinearities, if they are there. But do make sure to standardize your variables before applying KNN, or at least do something that accounts for the large differences in scale across the different variables here.

Which model seems to do better at achieving lower out-of-sample mean-squared error? Write a report on your findings as if you were describing your price-modeling strategies for a local taxing authority, who needs to form predicted market values for properties in order to know how much to tax them. Keep the main focus on the conclusions and model performance; any relevant technical details should be put in an appendix.

Answer: To create the KNN model I used the kfold cross validation technique using twenty different folds of data and fourty different k values that start at two and increase by two until reaching eighty. 
```{r echo=FALSE,warning=FALSE, message=FALSE}
#Create the best KNN model for price -> compare to lm from Saratoga lm
#Standardize variables excepet Class
SaratogaHouses_scale = SaratogaHouses #%>%
  # mutate_at(c('price', 'lotSize', 'age', 'landValue', 'livingArea', 'pctCollege', 'bedrooms', 'bathrooms', 'fireplaces', 'rooms'), funs(c(scale(.))))

#SaratogaHouses_scale <- SaratogaHouses_scale[c(-17,-18,-19)] # drop interactions for this portion



#saratoga_split_scale = initial_split(SaratogaHouses_scale, prop = 0.8)
#saratoga_train_scale = training(saratoga_split_scale)
#saratoga_test_scale = testing(saratoga_split_scale)

#rmse_out_scale = foreach(i=1:20, .combine='rbind') %dopar% {
#saratoga_split_scale = initial_split(SaratogaHouses_scale, prop = 0.8)
#saratoga_train_scale = training(saratoga_split_scale)
#saratoga_test_scale = testing(saratoga_split_scale)

    # train the model and calculate RMSE on the test set
    #lm_model_scale = lm(price ~ . - pctCollege - sewer - waterfront - landValue - newConstruction, data=saratoga_train_scale)
   #  this_rmse_scale = modelr::rmse(lm_model_scale, saratoga_test_scale)
  #}

#rmse_out_scale_mean = mean(rmse_out_scale)
#rmse_out_scale_mean
#0.6775124		
```

The KNN technique allowed me to choose the optimal value of K using the figure above. After using k=16 for my predication model I found that the KNN model significantly outperforms the standadized RMSEs for both the best linear model and the medium model. The standardized RMSE for the KNN model can be found below. 
```{r echo=FALSE, message = FALSE, warning=FALSE}
#Use KNN cross validation for various samples
SaratogaHouses_scale <- SaratogaHouses_scale[c(-17,-18,-19)] # drop interactions for this portion

# K-fold cross validation
K_folds = 20
SaratogaHouses_folds = crossv_kfold(SaratogaHouses_scale, k=K_folds)
# create a grid of K values -- the precise grid isn't important as long
# as you cover a wide range
k_grid = seq(2, 80, by=2)

# For each value of k, map the model-fitting function over the folds
# Using the same folds is important, otherwise we're not comparing
# models across the same train/test splits
cv_grid = foreach(k = k_grid, .combine='rbind') %do% {
models = map(SaratogaHouses_folds$train, ~ knnreg(price ~ lotSize + age + landValue + livingArea + pctCollege + bedrooms + bathrooms + fireplaces + rooms + heating + fuel + centralAir + sewer + waterfront, k=k, data = ., use.all=FALSE))
errs = map2_dbl(models, SaratogaHouses_folds$test, modelr::rmse)
c(k=k, err = mean(errs), std_err = sd(errs)/sqrt(K_folds))
} %>% as.data.frame

# RMSE for standardized price for k = 16
bestk = cv_grid %>%
  filter(k == 16)

bestk$err
# 0.6158611

# plot means and std errors versus k
ggplot(cv_grid) +
geom_point(aes(x=k, y=err)) +
geom_errorbar(aes(x=k, ymin = err-std_err, ymax = err+std_err)) +
labs(y="RMSE", title="RMSE vs k for KNN regression: Housing Prices")


```
```{r echo=FALSE, message=FALSE, warning=FALSE}
# fit at optimal k to show predicts on full data set
k_best = k_grid[which.min(cv_grid$err)]
knn_best = knnreg(price ~ lotSize + age + landValue + livingArea + pctCollege + bedrooms + bathrooms + fireplaces + rooms + heating + fuel + sewer + waterfront + newConstruction + centralAir, k=k_best, data = SaratogaHouses_scale)
# add predictions to data frame
SaratogaHouses_scale = SaratogaHouses_scale %>%
mutate(price_pred = predict(knn_best, SaratogaHouses_scale))


```
In conclusion, as a local tax authority, I recommend using the KNN model to predict market values for properties in order to know how much to tax them.


\newpage
### Question 3 
Consider the data in german_credit.csv on loan defaults from a German bank. The outcome variable of interest in this data set is default: a 0/1 indicator for whether a loan fell into default at some point before it was paid back to the bank. All other variables are features of the loan or borrower that might, in principle, help the bank predict whether a borrower is likely to default on a loan.

This data was collected in a retrospective, "case-control" design. Defaults are rare, and so the bank sampled a set of loans that had defaulted for inclusion in the study. It then attempted to match each default with similar sets of loans that had not defaulted, including all reasonably close matches in the analysis. This resulted in a substantial oversampling of defaults, relative to a random sample of loans in the bank's overall portfolio.

Of particular interest here is the "credit history" variable (history), in which a borrower's credit rating is classified as "Good", "Poor," or "Terrible." Make a bar plot of default probability by credit history, and build a logistic regression model for predicting default probability, using the variables duration + amount + installment + age + history + purpose + foreign.

What do you notice about the history variable vis-a-vis predicting defaults? What do you think is going on here? In light of what you see here, do you think this data set is appropriate for building a predictive model of defaults, if the purpose of the model is to screen prospective borrowers to classify them into "high" versus "low" probability of default? Why or why not---and if not, would you recommend any changes to the bank's sampling scheme?

```{r echo=FALSE, warning=FALSE, message=FALSE}
german_credit = read_csv('/Users/josephherrera/Desktop/ECO395M/data/german_credit.csv')

# Find the default proability for each class of credit history
# Dont include the row, column, or chi-squared proportions in the table
#gc_prob <- CrossTable(german_credit$Default, german_credit$history, digits = 3, prop.r = F, prop.c = F, prop.chisq = F, prop.t = T)

#gc_prob

# Creating a variable that contains only the default probability
#data_relationship <- gc_prob$prop.tbl 
# Plotting it
#position <- data_relationship / 2
#text(x = barplot(data_relationship),labels=names(data_relationship), y = position)
#title("Default Probability of Deafulting on a Loan per Credit Class")

history_bar = german_credit %>%
  group_by(history) %>%
  summarise(n=n(), sumDefault=sum(Default), probDefault = sumDefault/n)

ggplot(history_bar) +
  geom_col(aes(x=history, y=probDefault, fill = history)) +
  xlab("Credit History Category") +
  ylab("Probability of Default")


```
Caption: The bar plot illustrates the probability of an individual defaulting on a loan based on their credit history. This is particularly interesting because individuals with good credit history are more likely to default on a loan than people from other credit history categories.


```{r echo=FALSE, message=FALSE, warning=FALSE }
#Create logisitic regression with default as the outcome variable
# Create the initial train and test splits
default_split = initial_split(german_credit, prop = 0.8)
default_train = training(default_split)
default_test = testing(default_split)

# Fit a logistic model to the training data and look at  the coefficients
log_default <- glm(Default ~ duration + amount + installment + age + history + purpose + foreign, family = 'binomial', data = default_train)

#coef(log_default) %>% round(digits = 2)

#Add predictions to the training model and compare it to the actual data
phat_train_default1 = predict(log_default, default_train)
yhat_train_default1 = ifelse(phat_train_default1 > 0.5, 1, 0)
confusion_in = table(y = default_train$Default, yhat = yhat_train_default1)
#confusion_in
#sum(diag(confusion_in))/sum(confusion_in)  # in-sample accuracy

#log_default <- glm(Default ~ duration + amount + installment + age + history + purpose + foreign, family = 'binomial', data = german_credit)

#coef(log_default) %>% round(digits = 2)
```
After predicting out-of-sample accuracy for the predictive logisitic model, you can see that there are very few observations where people default in the confusion matrix below. This leads to an out-of-sample accuracy that hovers around 70%.
```{r echo=FALSE, message=FALSE, warning=FALSE}
phat_test_default1 = predict(log_default, default_test)
yhat_test_default1 = ifelse(phat_test_default1 > 0.5, 1, 0)
confusion_out = table(y = default_test$Default, yhat = yhat_test_default1)
confusion_out  # confusion matrix
sum(diag(confusion_out))/sum(confusion_out)  # out-of-sample accuracy

#0.7135678
```
The null model has a 70% accuracy rating when guessing that no individual defaulted on their loan. Thus, the approximate 1.4% absolute increase in model accuracy with the logisitic model is marginal.

What do you notice about the history variable vis-a-vis predicting defaults? What do you think is going on here?
It seems the history variable does a poor job of predicting defaults. I think this is becuase individuals categorized as having poor or terrible credit are less likely to apply for or recieve loans. 

In light of what you see here, do you think this data set is appropriate for building a predictive model of defaults, if the purpose of the model is to screen prospective borrowers to classify them into "high" versus "low" probability of default? Why or why not---and if not, would you recommend any changes to the bank's sampling scheme?
I do not think this data is appropriate for building a predictive model of defaults because the accuracy rating of a logistical regression that includes important characteristic variables (duration + amount + installment + age + history + purpose + foreign) only out performs the null model by approximately 1.4%. This could occur because of the small number of individuals who default on loans at the German bank in the dataset. I recommend that German Bank not oversample defaults from their data. 

### Question 4

```{r echo=FALSE, warning=FALSE, message=FALSE}
# Load children and hotel reservations data
library(nnet)
hotels_dev = read_csv("/Users/josephherrera/Desktop/ECO395M/data/hotels_dev.csv")

hotels_val = read_csv("/Users/josephherrera/Desktop/ECO395M/data/hotels_val.csv")
```
Model building: Below is the code, respective confusion matrices, and out-of-sample accuracy for the three different models.
```{r echo=TRUE, warning=FALSE, message=FALSE}
#Model building

#Create the initial split for hotels_dev
hotels_dev_split = initial_split(hotels_dev, prop = 0.8)
hotels_dev_train = training(hotels_dev_split)
hotels_dev_test = testing(hotels_dev_split)

log_baseline1 = glm(children ~ market_segment + adults + customer_type + is_repeated_guest, data = hotels_dev_train, family = "binomial")

#coef(log_baseline1) %>% round(3)

#Add predictions and calculate the out-of-sample accuracy
phat_baseline1 = predict(log_baseline1, hotels_dev_test)
yhat_baseline1 = ifelse(phat_baseline1 > 0.5, 1, 0)
confusion_matrix_baseline1_out = table(y = hotels_dev_test$children, yhat = yhat_baseline1)

confusion_matrix_baseline1_out #confusion matrix

sum(diag(confusion_matrix_baseline1_out))/sum(confusion_matrix_baseline1_out)#out-of-sample accuracy

#0.9173241
```

At a threshold of 0.5, the small model with few covariates does not predict that any booking will have kids.

```{r message = FALSE, warning=FALSE, echo=TRUE}
#Create baseline model 2 

hotels_dev_split2 = initial_split(hotels_dev, prop = 0.8)
hotels_dev_train2 = training(hotels_dev_split2)
hotels_dev_test2 = testing(hotels_dev_split2)

log_baseline2 = glm(children ~ . - arrival_date, family = "binomial", data = hotels_dev_train2)

#coef(log_baseline2) %>% round(3)

#Add predictions and calculate the out-of-sample accuracy
phat_baseline2 = predict(log_baseline2, hotels_dev_test2)
yhat_baseline2 = ifelse(phat_baseline2 > 0.5, 1, 0)
confusion_matrix_baseline2_out = table(y = hotels_dev_test2$children, yhat = yhat_baseline2)

confusion_matrix_baseline2_out #confusion matrix

sum(diag(confusion_matrix_baseline2_out))/sum(confusion_matrix_baseline2_out)#out-of-sample accuracy

#0.9329926
```

The large model's confusion matrix (found above) is effective at predicting booking that have children and has an out-of-sample accuracy that is continuously over 90%.


```{r echo=TRUE, message=FALSE, warning=FALSE}
#Create the best linear model possible 
hotels_dev_step_split = initial_split(hotels_dev, prop = 0.8)
hotels_dev_step_train = training(hotels_dev_step_split)
hotels_dev_step_test = testing(hotels_dev_step_split)

#Create medium baseline model using baseline model 1
lm_medium = lm(children ~ market_segment + adults + customer_type + is_repeated_guest, data = hotels_dev_step_train)

#Find the best linear model
library(gamlr) #For the Lasso regression

####Use the Lasso regression
## full model
full = glm(children ~ ., data=hotels_val, family=binomial)

#Create numeric feature matrix.
scx = model.matrix(children ~ .-1, data=hotels_val) # do -1 to drop intercept!
scy = hotels_val$children

# Cross validation lasso regression
sccvl = cv.gamlr(scx, scy, nfold=20, family="binomial", verb=F)

#plot(sccvl, bty="n")

## CV min deviance selection
#scb.min = coef(sccvl, select="min")
#log(sccvl$lambda.min)
#sum(scb.min!=0) # note: this is random!  because of the CV randomness

#scbeta = coef(sccvl) 
#scbeta



lm0 = lm(children ~ hotel + lead_time + adults + meal + distribution_channel + is_repeated_guest + market_segment + reserved_room_type + previous_bookings_not_canceled + booking_changes + customer_type + average_daily_rate  + total_of_special_requests + arrival_date, data = hotels_dev_step_train)

#Find out-of-sample accuracy
phat_step = predict(lm0, hotels_dev_step_test)
yhat_step = ifelse(phat_step > 0.5, 1, 0)
confusion_matrix_step_out = table(y = hotels_dev_step_test$children, yhat = yhat_step)

confusion_matrix_step_out #confusion matrix

sum(diag(confusion_matrix_step_out))/sum(confusion_matrix_step_out)#out-of-sample accuracy

# 0.9175464
```

To create the best linear model, I used a lasso regression with cross validation to find the which parameters should be in the linear model. The out-of-sample accuracy for this model consistently outperforms the second baseline model by half a percentage point.

\newpage
Model Validation Step 1:
Once you've built your best model and assessed its out-of-sample performance using hotels_dev, now turn to the data in hotels_val. Now you'll validate your model using this entirely fresh subset of the data, i.e. one that wasn't used to fit OR test as part of the model-building stage. (Using a separate "validation" set, completely apart from your training and testing set, is a generally accepted best practice in machine learning.)

Produce an ROC curve for your best model, using the data in hotels_val: that is, plot TPR(t) versus FPR(t) as you vary the classification threshold t.
```{r echo=TRUE , warning=FALSE, message=FALSE}
#Model Validation Step 1

#Create the train-test split
#hotels_val_split = initial_split(hotels_val, prop=0.8)
#otels_val_train = training(hotels_val_split)
#hotels_val_test = testing(hotels_val_split)

#Fit the model with the training data
#lm_val = lm(children ~ market_segment + adults + customer_type + is_repeated_guest + 
#    market_segment:adults + adults:customer_type + market_segment:is_repeated_guest, data = hotels_val_train)

#coef(lm_val) %>% round(3)

#Add predictions and calculate the out-of-sample accuracy
phat_val = predict(lm0, hotels_val)
yhat_val = ifelse(phat_val > 0.15, 1, 0)
confusion_matrix_val_out = table(y = hotels_val$children, yhat = yhat_val)

confusion_matrix_val_out #confusion matrix

sum(diag(confusion_matrix_val_out))/sum(confusion_matrix_val_out)#out-of-sample accuracy

# 0.9029029

```
After estimating the accuracy of the predicted logistic model on the hotels_val dataset, we see that the accuracy hovers below 90%.



```{r echo = FALSE, warning=FALSE, message=FALSE}
#Create the ROC curve using the hotels_val data
phat_val = predict(lm0, hotels_val, type = "response")

thresh_grid = seq(0.15, 0.05, by=-0.001)
roc_curve_val = foreach(thresh = thresh_grid, .combine='rbind') %do% {
 yhat_val = ifelse(phat_val > thresh, 1, 0)

  # FPR, TPR for linear model
 confusion_matrix_val_out = table(y = hotels_val$children, yhat = yhat_val)
  out_lin = data.frame(model = "linear",
                       TPR = confusion_matrix_val_out[2,2]/sum(hotels_val$children==1),
                       FPR = confusion_matrix_val_out[1,2]/sum(hotels_val$children==0))
  rbind(out_lin)
} %>% as.data.frame()
ggplot(data = roc_curve_val) + 
  geom_line(aes(x=FPR, y=TPR, color = model)) + 
  labs(title="ROC curve: model of best accuracy") +
  theme_bw(base_size = 10) +
  xlim(0,0.5) +
  ylim(0,1)
```

Caption: The ROC curve depicts the true positive rate and false positive rate for values of threshold t. 

\newpage
Model Validation Step 2:
Next, create 20 folds of hotels_val. There are 4,999 bookings in hotels_val, so each fold will have about 250 bookings in it -- roughly the number of bookings the hotel might have on a single busy weekend. 

```{r echo=FALSE}
#Model Validation Step 2
library(foreach)
# allocate to folds
N = nrow(hotels_val)
K = 20
fold_id = rep_len(1:K, N)  # repeats 1:K over and over again
fold_id = sample(fold_id, replace=FALSE) # permute the order randomly

base3hand_val = lm(children~ hotel+lead_time+adults+meal+market_segment+distribution_channel+is_repeated_guest+
                     previous_bookings_not_canceled+reserved_room_type+booking_changes+customer_type+average_daily_rate+
                     total_of_special_requests+arrival_date, data = hotels_val)

hotels_val_folds <- hotels_val %>%
  mutate(fold_id = sample(fold_id, replace=FALSE), phat_val_test = predict(base3hand_val, hotels_val, type='response'), yhat_val_test = ifelse(phat_val_test > 0.5, 1, 0)) #%>%

fold_groups <- hotels_val_folds%>%
  group_by(fold_id)%>%
  summarize(prop_children = mean(children), prop_pred = mean(phat_val_test), count_children = sum(children), count_pred = sum(yhat_val_test) )

fold_groups

```
The table above indicates that this model is inconsistent at predicting the the total number of bookings in a group of 250 bookings. Since the number of bookings with children are small, over or under predicting by ten children could misinform hotel management on how many chicken nuggets they should purchase in the given timeframe.









