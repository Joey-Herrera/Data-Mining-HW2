---
title: "DM_Excersize2_Q1"
author: "Joey Herrera"
date: "2/10/2021"
output: pdf_document
---

```{r setup, include=FALSE, message=FALSE}
library(tidyverse)
library(ggplot2)
library(lubridate)
library(rsample)  # for creating train/test splits
library(caret)
library(modelr) 
library(parallel) #run things at the sametime
library(foreach) #loop

capmetro = read_csv('/Users/josephherrera/desktop/ECO395M/data/capmetro_UT.csv')
```

# Data Mining Assignment 2

### Question 1: Data Visualization
One panel of line graphs that plots average boardings grouped by hour of the day, day of week, and month. You should facet by day of week. Each facet should include three lines, one for each month, colored differently and with colors labeled with a legend. 

Give the figure an informative caption in which you explain what is shown in the figure and address the following questions, citing evidence from the figure. Does the hour of peak boardings change from day to day, or is it broadly similar across days? Why do you think average boardings on Mondays in September look lower, compared to other days and months? Similarly, why do you think average boardings on Weds/Thurs/Fri in November look lower?
```{r echo=FALSE}
#Generate a variable calculating the average boardings group
capmetro_timestamp = capmetro %>%
  mutate(day_of_week = factor(day_of_week,
                 levels=c("Mon", "Tue", "Wed","Thu", "Fri", "Sat", "Sun")),
               month = factor(month,
                 levels=c("Sep", "Oct","Nov")),
         timestamp = ymd_hms(timestamp)) %>%
    group_by(hour_of_day, day_of_week, month) %>%
  summarise( avg_boarding = mean(boarding))

#Dummy variable encoding
capmetro_matrix = capmetro_timestamp %>%
  model_matrix(~avg_boarding + month)

#Join both matricies by avg_boarding
capmetro_facet <- merge(capmetro_timestamp, capmetro_matrix, by="avg_boarding")
#Add an additional monthSep var so I dont need to graph the intercept
capmetro_facet = capmetro_facet %>%
  mutate(monthSep = ifelse(monthOct & monthNov == 0, 0 ,1)) #The zeros correpsond to the values that monthOct and monthNov must be for monthSep to equal 1

# Create the first faceted line plot
ggplot(data = capmetro_facet) +
  geom_line(aes(x= hour_of_day, y= avg_boarding, color = month)) + #Color can add an addtional variable to the plot
  facet_wrap(~ day_of_week) +
  xlab("Hour of the Day") +
  ylab("Average Boarding") +
  theme_bw()
```

One panel of scatter plots showing boardings (y) vs. temperature (x) in each 15-minute window, faceted by hour of the day, and with points colored in according to whether it is a weekday or weekend. 

Give the figure an informative caption in which you explain what is shown in the figure and answer the following question, citing evidence from the figure. When we hold hour of day and weekend status constant, does temperature seem to have a noticeable effect on the number of UT students riding the bus?

```{r echo=FALSE}
capmetro_scatter = capmetro %>%
  mutate(day_of_week = factor(day_of_week,
                 levels=c("Mon", "Tue", "Wed","Thu", "Fri", "Sat", "Sun")),
               month = factor(month,
                 levels=c("Sep", "Oct","Nov")),
         timestamp = ymd_hms(timestamp)) %>%
  group_by(timestamp, weekend) 

ggplot(data = capmetro_scatter) +
  geom_point(aes(x=temperature, y=boarding, color = weekend)) +
  facet_wrap(~ hour_of_day)
```

## Question 2: Saratoga Housing Prices

Return to the data set on house prices in Saratoga, NY that we considered in class. Recall that a starter script here is in saratoga_lm.R. For this data set, you'll run a "horse race" (i.e. a model comparison exercise) between two model classes: linear models and KNN.

Build the best linear model for price that you can. It should clearly outperform the "medium" model that we considered in class. Use any combination of transformations, engineering features, polynomial terms, and interactions that you want; and use any strategy for selecting the model that you want.
```{r echo=FALSE}
#Load Saratoga houses data
data(SaratogaHouses)

#Linear models from class
# Split into training and testing sets
saratoga_split = initial_split(SaratogaHouses, prop = 0.8)
saratoga_train = training(saratoga_split)
saratoga_test = testing(saratoga_split)
	
# Fit to the training data
# Sometimes it's easier to name the variables we want to leave out
# The command below yields exactly the same model.
# the dot (.) means "all variables not named"
# the minus (-) means "exclude this variable"
lm1 = lm(price ~ lotSize + bedrooms + bathrooms, data=saratoga_train)
lm2 = lm(price ~ . - pctCollege - sewer - waterfront - landValue - newConstruction, data=saratoga_train)
lm3 = lm(price ~ (. - pctCollege - sewer - waterfront - landValue - newConstruction)^2, data=saratoga_train)

# Predictions out of sample
# Root mean squared error
rmse(lm1, saratoga_test)
rmse(lm2, saratoga_test)
rmse(lm3, saratoga_test)

```

```{r echo=FALSE, warning=FALSE}
#Add additional variables to the best linear model (BLM)
SaratogaHouses = SaratogaHouses %>%
  mutate(livingArea_rooms = livingArea*rooms,
         livingArea_bathrooms = livingArea*bathrooms,
         livingArea_fireplaces = livingArea*fireplaces)
        # log_age = log(age)) #cannot use log_age because some observation have age = 0

saratoga_split_lm = initial_split(SaratogaHouses, prop = 0.8)
saratoga_train_lm = training(saratoga_split_lm)
saratoga_test_lm = testing(saratoga_split_lm)

rmse_out_lm = foreach(i=1:10, .combine='rbind') %dopar% {
saratoga_split_lm = initial_split(SaratogaHouses, prop = 0.8)
saratoga_train_lm = training(saratoga_split_lm)
saratoga_test_lm = testing(saratoga_split_lm)
  this_rmse_lm = foreach(i = 1:10, .combine='c') %do% {
    # train the model and calculate RMSE on the test set
    lm_model = lm(price ~ . - pctCollege - sewer - waterfront - landValue - newConstruction, data=saratoga_train_lm, i=1:10, use.all=TRUE)
    modelr::rmse(lm_model, saratoga_test_lm)
  }
  data.frame(i=c(1:10), rmse=this_rmse_lm)
}
rmse_out_lm = arrange(rmse_out_lm, i)

rmse_out_lm_final = rmse_out_lm %>%
  group_by(as.factor(i)) %>%
  summarize(avg_rmse = mean(rmse))

rmse_out_lm_final

#Create the best linear model for price
#Add additional variables to the best linear model (BLM)
saratoga_split_blm = initial_split(SaratogaHouses, prop = 0.8)
saratoga_train_blm = training(saratoga_split_blm)
saratoga_test_blm = testing(saratoga_split_blm)

rmse_out_blm = foreach(i=1:10, .combine='rbind') %dopar% {
saratoga_split_blm = initial_split(SaratogaHouses, prop = 0.8)
saratoga_train_blm = training(saratoga_split_blm)
saratoga_test_blm = testing(saratoga_split_blm)
  this_rmse_blm = foreach(i = 1:10, .combine='c') %do% {
    # train the model and calculate RMSE on the test set
    blm_model = lm(price ~ lotSize + landValue + age + livingArea + bedrooms + fireplaces + bathrooms + rooms + heating + fuel + centralAir + livingArea_rooms+ livingArea_bathrooms + livingArea_fireplaces, data=saratoga_train_blm, i=1:10, use.all=TRUE)
    modelr::rmse(blm_model, saratoga_test_blm)
  }
  data.frame(i=c(1:10), rmse=this_rmse_blm)
}
rmse_out_blm = arrange(rmse_out_blm, i)

rmse_out_blm_final = rmse_out_blm %>%
  group_by(as.factor(i)) %>%
  summarize(avg_rmse = mean(rmse))

rmse_out_blm_final



###### Test linear model
SaratogaHouses = SaratogaHouses %>%
  mutate(livingArea_rooms = livingArea*rooms,
         livingArea_bathrooms = livingArea*bathrooms,
         livingArea_fireplaces = livingArea*fireplaces)
        # log_age = log(age)) #cannot use log_age because some observation have age = 0
 

test_model = lm(price ~ lotSize + landValue + age + livingArea + bedrooms + fireplaces + bathrooms + rooms + heating + fuel + centralAir + livingArea_rooms+ livingArea_bathrooms + livingArea_fireplaces, data = saratoga_train_blm)

modelr::rmse(test_model, saratoga_test_blm)
 #RMSE first try 58693.66
```


Now build the best K-nearest-neighbor regression model for price that you can. Note: you still need to choose which features should go into a KNN model, but you don't explicitly include interactions or polynomial terms. The method is sufficiently adaptable to find interactions and nonlinearities, if they are there. But do make sure to standardize your variables before applying KNN, or at least do something that accounts for the large differences in scale across the different variables here.

Which model seems to do better at achieving lower out-of-sample mean-squared error? Write a report on your findings as if you were describing your price-modeling strategies for a local taxing authority, who needs to form predicted market values for properties in order to know how much to tax them. Keep the main focus on the conclusions and model performance; any relevant technical details should be put in an appendix.
```{r echo=FALSE}
#Create the best KNN model for price -> compare to lm from Saratoga lm
#Standardize variables

#Use KNN cross validation for various samples




```

